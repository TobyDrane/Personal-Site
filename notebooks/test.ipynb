{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement \n",
    "\n",
    "Suppose we are playing a game, let's play a simple coin slot machine, the ones like we used to play as kids on the beach. For now we only have one slot\n",
    "in the machine where we can drop our penny. We drop our penny and we will recieve some reward, in our world this is a numerical value. For instance we have dropped our\n",
    "penny such that we have knocked another three pennies off, thus the reward for that action can be formalised as +3. Our objective is to come out of the slot machine\n",
    "room with more pennies than we started with, ideally holding every penny that we could possibly win within a set timeframe. \n",
    "\n",
    "Our penny machines normall have more than one slot, we can extend our problem to whats known as k-armed Bandits. We have k slots, where the action we take is which slot to play\n",
    "and the reward is how many pennies did we win on that action.\n",
    "\n",
    "Each action within our bandit as an expected reward for that given selected action, this is sometimes known as the action-value. Where value is another word for our reward. \n",
    "At some timestep $t$, we selected an action $A_t$, and recieve some numerical reward $R_t$. For an arbitary action our action-value can be stated as:\n",
    "\n",
    "$$\n",
    "q(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "$$\n",
    "\n",
    "One can see that if we known all the values for every action, the solution is as simple as choosing the action with the highest value.\n",
    "We do not know the exact action-values, instead we have an estimate $Q_t(a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living life as a Bandit\n",
    "\n",
    "Within our algorithm we keep track of our action-value estimates $Q_t(a)$, infact this what we are trying to solve. At every step we have a action value that is greater than another.\n",
    "If you as a player have been consistently adding coins to the right hand side of the slot machine you probably have a higher chance of winning coins on the right hand side of the machine and so\n",
    "the action-values will correspond to that notion. Choosing an action from these slots on the right hand side of the machine is **exploiting** your underlying knowledge of the world.\n",
    "\n",
    "Now your friend turns up and annoys you by placing a coin on the left hand side of the machine. This goes against your internal action-value map that you have. These are not the current highest\n",
    "estimates. What your friend has just done is **explore** the world. For those who are reading who come from a machine learning world, this is the same concept as a gradient descent.\n",
    "We have a world with potentially many avenues, all giving potentially different rewards. We then have a **conflict** between how much do we want to explore this world and recieve maybe some low\n",
    "values, or how long do we want to keep **exploiting** the avenue we are currently in knowing we have a good reward (alas potentially not the best).\n",
    "\n",
    "We shall take a look at some different algorithms which way up different versions of this conflict problem and show some nice results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Formalisation\n",
    "\n",
    "## Incremental action-value\n",
    "\n",
    "We now focus on a single action. How can we calculate the action-value after $n$ rewards. Well this is simply the estimate after it has selection $n - 1$ rewards.\n",
    "\n",
    "$$\n",
    "Q_n = \\frac{R_1 + R_2 + ... + R_{n-1}}{n-1}\n",
    "$$\n",
    "\n",
    "However for this we will need to store all the previous rewards. How can we do it incrementally? Well the solution is to store a running sun and count.\n",
    "\n",
    "$$\n",
    "Q_{n+1} = Q_n + \\frac{1}{n} [R_n - Q_n]\n",
    "$$\n",
    "\n",
    "I'm not going to show the proof within here but I would urge the reader to go and read a great book by Sutton and Barto, Reinforcement Learning an Introduction.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "The base algorithm can be seen in the pseudocode for incremental updates. Note, this is the algorithm for an epsilon greedy bandit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Bandit\n",
    "\n",
    "The first bandit we will describe is a simple random bandit. This bandit has no care about what previous actions have been taken, nor what previous rewards have been achieved.\n",
    "The random bandit algorithm Python code can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random(object):\n",
    "  \"\"\"A random agent.\n",
    "\n",
    "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
    "  random. The 'previous_action' argument of 'step' is ignored.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, name, number_of_arms):\n",
    "    \"\"\"Initialise the agent.\n",
    "    \n",
    "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
    "    bandits `arm` is just another word for `action`.)\n",
    "    \"\"\"\n",
    "    self._number_of_arms = number_of_arms\n",
    "    self.name = name\n",
    "\n",
    "  def step(self, unused_previous_action, unused_reward):\n",
    "    \"\"\"Returns a random action.\n",
    "    \n",
    "    The inputs are ignored, but this function still requires an action and a\n",
    "    reward, to have the same interface as other agents who may use these inputs\n",
    "    to learn.\n",
    "    \"\"\"\n",
    "    return np.random.randint(self._number_of_arms)\n",
    "\n",
    "  def reset(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Greedy\n",
    "\n",
    "The epsilon greedy algorithm allows the bandit to balance the exploration vs exploitation conflict that we mentioned earlier. We introduce the parameter $\\epsilon$ which is the probability\n",
    "of choosing whether to explore, or exploit. If the proability "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('3.10.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66e25b169da7357878dccb750fd84445694c94b0dfceae307281a45c53656c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
